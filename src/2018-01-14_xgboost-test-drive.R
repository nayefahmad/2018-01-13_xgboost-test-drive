

#******************************
# TESTING XGBOOST PACKAGE 
#******************************

help(package = "xgboost")
# reference: https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html

require(xgboost)
require(Matrix)
require(data.table)
if (!require('vcd')) install.packages('vcd')

# TODO: -----------------------

#******************************




# load arthritis data: 
data(Arthritis)
df <- data.table(Arthritis, 
                 keep.rownames = FALSE)

head(df)
str(df)


# experiment1: group ages into buckets of 10: -----
head(df[ , AgeDiscret := as.factor(round(Age/10, 0))])

# note: above we're using data.table syntax. 
# General form: DT[i, j, by]
# “Take DT, subset rows using i, then calculate j grouped by by”
# ":=" is used to update cols by reference to name

head(df); str(df)  
# note that we have modified df, without having to use an 
# assignment statement like "df <- df[i,j,by]

# experiment2: arbitrarily group ages into 2 buckets: 
head(df[ , AgeCat := as.factor(ifelse(Age > 30, 
                                      "Old", 
                                      "Young"))])

# note that these new cols are highly correlated with Age. 
# This is not a problem for a decision tree (unlike a GLM)

head(df); str(df)

# remove col ID: 
df[ , ID := NULL]

# transform categorical col "Treatment" to dummy vars: ----
# levels of treatment: 

# First let's take a look at the levels of the factor cols: 
levels(df[ , Treatment])
levels(df[ , Improved])

# create a sparse matrix by changing factors to binary cols: 
sparse_matrix <- sparse.model.matrix(Improved~., 
                                     data = df)[,-1]
# note that we're not changing col "Improved", bcoz that's 
#     the thing we want to predict. 


head(sparse_matrix)
# str(sparse_matrix)

# The [,-1]is here to remove the first column which
# is full of 1 (this column is generated by the conversion
# formula "Improved ~ ."?). 
# For more information, you can type ?sparse.model.matrix 
# in the console.

# Create the output numeric vector (not as a sparse Matrix):
output_vector <- df[ , Improved] == "Marked"
# returns a logical vec with TRUE in those cases where
# Improved col = "Marked" 


# build the model: ----------
bst <- xgboost(data = sparse_matrix,  # input data  
               label = output_vector,  # vector of response vals
               max_depth = 4,  # max tree depth 
               eta = 1,  # control the learning rate: scale the contribution of each tree by a factor of 0 < eta < 1 when it is added to the current approximation. Used to prevent overfitting by making the boosting process more conservative. 
               nthread = 2,  # todo: 
               nrounds = 10,  # max iterations  
               objective = "binary:logistic")  # specify the learning task and the corresponding learning objective, users can pass a self-defined function to it. 

# what are those args? 
?xgboost
# fn xgboost returns object of class xgb.Booster


# feature importance: ------
importance <- xgb.importance(feature_names = colnames(sparse_matrix),
                             model = bst)
head(importance)

?xgb.importance

# Gain is the improvement in accuracy brought by a 
# feature to the branches it is on. The idea is that 
# before adding a new split on a feature X to the 
# branch there was some wrongly classified elements, 
# after adding the split on this feature, there are 
# two new branches, and each of these branch is more 
# accurate (one branch saying if your observation is 
# on this branch then it should be classified as 1, and
# the other branch saying the exact opposite).


# For boosted tree model, each gain of each feature 
# of each tree is taken into account, then average per 
# feature to give a vision of the entire model. Highest
# percentage means important feature to predict the 
# label used for the training (only available for tree 
# models);

# Cover measures the relative quantity of observations
# concerned by a feature.

# Frequency is a simpler way to measure the Gain. It 
# just counts the number of times a feature is used in 
# all generated trees. You should not use it (unless 
# you know why you want to use it).

# plotting importance: ------
xgb.plot.importance(importance_matrix = importance)


# feature importance with splits: -----
# the info above is useful, but not enough. We know that age 
# is useful for predicting Improved, but what is the exact 
# relationship? e.g. age>60 means Improved == None? 

# to get this info we use xgb.importance with 2 more args: 
importanceRaw <- xgb.importance(feature_names = colnames(sparse_matrix),
                                model = bst,
                                data = sparse_matrix, 
                                label = output_vector)
# importanceRaw

# Cleaning for better display: 
importanceClean <- importanceRaw[ ,`:=`(Cover=NULL, 
                                        Frequency=NULL)]
head(importanceClean)

# TODO: I don't see any of these columns mentioned 
# below: 

# RealCover and RealCover %: In the first column it 
# measures the number of observations in the dataset
# where the split is respected and the label marked
# as 1.
# todo: what does label = 1 mean? 

# The second column is the percentage of the whole
# population that RealCover represents.

# Therefore, according to our findings, getting a 
# placebo doesn't seem to help but being younger than
# 61 years may help (seems logical).


# check: do the results make sense? -----

# do a chisq test to see if Age is really statistically 
# related to the outcome we're interested in (the output
# vec "output_vector")

chisq.test(df$Age, output_vector)

# p-value = 0.4458
# reject null that Age has no impact on outcome 



# special note: what about Random Forests? -----
# Both trains several decision trees for one dataset.
# The main difference is that in Random Forests™, trees
# are independent and in boosting, the tree N+1 focus 
# its learning on the loss (<=> what has not been well 
# modeled by the tree N).

# This difference have an impact on a corner case in feature
# importance analysis: the correlated features.

