

#******************************
# TESTING XGBOOST PACKAGE 
#******************************

help(package = "xgboost")

require(xgboost)
require(Matrix)
require(data.table)
if (!require('vcd')) install.packages('vcd')

# TODO: -----------------------

#******************************




# load arthritis data: 
data(Arthritis)
df <- data.table(Arthritis, keep.rownames = FALSE)

head(df)
str(df)


# experiment1: group ages into buckets of 10: -----
head(df[ , AgeDiscret := as.factor(round(Age/10, 0))])

# note: above we're using data.table syntax. 
# General form: DT[i, j, by]
# “Take DT, subset rows using i, then calculate j grouped by by”
# ":=" is used to update cols by reference to name

# experiment2: arbitrarily group ages into 2 buckets: 
head(df[ , AgeCat := as.factor(ifelse(Age > 30, 
                                      "Old", 
                                      "Young"))])

# note that these new cols are highly correlated with Age. 
# This is not a problem for a decision tree (unlike a GLM)

# remove col ID: 
df[ , ID := NULL]

# transform categorical col "Treatment" to dummy vars: ----
# levels of treatment: 
levels(df[ , Treatment])
# levels(df[ , Improved])

sparse_matrix <- sparse.model.matrix(Improved~.-1, data = df)
# note that we're not changing col "Improved", bcoz that's 
#     the thing we want to predict. 

head(sparse_matrix)
# Formulae Improved~.-1 used above means transform all categorical features but column Improved to binary values. The -1 is here to remove the first column which is full of 1 (this column is generated by the conversion). For more information, you can type ?sparse.model.matrix in the console.


output_vector <- df[ , Improved] == "Marked"



# build the model: ----------
bst <- xgboost(data = sparse_matrix,  # input data  
               label = output_vector,  # vector of response vals
               max_depth = 4,  # todo: 
               eta = 1,  # todo: 
               nthread = 2,  # todo: 
               nrounds = 10,  # max iterations  
               objective = "binary:logistic")  # todo: 

# what are those args? 
?xgboost
# fn xgboost returns object of class xgb.Booster


# feature importance: ------
importance <- xgb.importance(feature_names = colnames(sparse_matrix),
                             model = bst)
head(importance)

# Gain is the improvement in accuracy brought by a feature to the branches it is on. The idea is that before adding a new split on a feature X to the branch there was some wrongly classified elements, after adding the split on this feature, there are two new branches, and each of these branch is more accurate (one branch saying if your observation is on this branch then it should be classified as 1, and the other branch saying the exact opposite).

# For boosted tree model, each gain of each feature of each tree is taken into account, then average per feature to give a vision of the entire model. Highest percentage means important feature to predict the label used for the training (only available for tree models);

# Cover measures the relative quantity of observations concerned by a feature.

# Frequency is a simpler way to measure the Gain. It just counts the number of times a feature is used in all generated trees. You should not use it (unless you know why you want to use it).

?xgb.importance


# feature importance with splits: -----
# the info above is useful, but not enough. We know that age 
# is useful for predicting Improved, but what is the exact 
# relationship? e.g. age>60 means Improved == None? 

# to get this info we use xgb.importance with 2 more args: 
importanceRaw <- xgb.importance(feature_names = colnames(sparse_matrix),
                                model = bst,
                                data = sparse_matrix, 
                                label = output_vector)

# Cleaning for better display: 
importanceClean <- importanceRaw[ ,`:=`(Cover=NULL, Frequency=NULL)]
# todo: not entirely sure what `:=` does 

head(importanceClean)

# RealCover and RealCover %: In the first column it measures the number of observations in the dataset where the split is respected and the label marked as 1.
#     > todo: what does label = 1 mean? 

# The second column is the percentage of the whole population that RealCover represents.

# Therefore, according to our findings, getting a placebo doesn't seem to help but being younger than 61 years may help (seems logic).
